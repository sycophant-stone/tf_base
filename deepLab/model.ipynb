{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## useful model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-6e9a8b589fb5>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-6e9a8b589fb5>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    def _get_logits(images,model_options,weight_decay=0.0001,is_train)\u001b[0m\n\u001b[0m                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "slim=tf.contrib.slim\n",
    "\n",
    "LOGITS_SCOPE_NAME = 'logits'\n",
    "MERGED_LOGITS_SCOPE = 'merged_logits'\n",
    "IMAGE_POOLING_SCOPE = 'image_pooling'\n",
    "ASPP_SCOPE = 'aspp'\n",
    "CONCAT_PROJECTION_SCOPE = 'concat_projection'\n",
    "DECODER_SCOPE = 'decoder'\n",
    "\n",
    "def cal_scaled_dim_val(dim,scale_coeff):\n",
    "    \"\"\"利用scale_coeff对dim维做成scale的维度.\n",
    "       这里只是一个计算,计算scale之后的维度数.其实并没有实际scale数据Tensor对象.\n",
    "    \"\"\"\n",
    "    if isinstance(dim,tf.Tensor):\n",
    "        return tf.cast((tf.tofloat(dim)-1.0)*scale_coeff+1.0,tf.int32) # 其实这里边的这个+1.0是为了向上取整\n",
    "    else:\n",
    "        return (float(dim)-1.0)*scale_coeff+1.0\n",
    "\n",
    "def _get_logits(images,model_options,weight_decay=0.0001,reuse=None,is_training=False,\n",
    "               find_tune_batch_norm=False):\n",
    "    \"\"\"生成logits网络.该网络应用到aspp,atrous spatial pyramid pooling.\n",
    "    \"\"\"\n",
    "    # 提取features和end_points.\n",
    "    features,end_points=extract_features(\n",
    "        images,\n",
    "        model_options,\n",
    "        weight_decay=weight_decay,\n",
    "        reuse=reuse,\n",
    "        is_training=is_training,\n",
    "        find_tune_batch_norm=find_tune_batch_norm)\n",
    "    \n",
    "    # 如果decoder 有特殊定义的stride.需要对decoder size做scale\n",
    "    if model_option.decoder_output_stride is not None:\n",
    "        if model_option.crop_size is None:\n",
    "            height=tf.shape(images)[1]\n",
    "            width=tf.shape(images)[2]\n",
    "        else:\n",
    "            # crop存在\n",
    "            height,width=model_option.crop_size\n",
    "        \n",
    "        # 求decoder使用的size.这个是经过decoder_output_stride之后的.\n",
    "        decoder_height=cal_scaled_dim_val(height,1.0/model_options.decoder_output_stride)\n",
    "        decoder_width=cal_scaled_dim_val(width,1.0/model_options.decoder_output_stride)\n",
    "        \n",
    "        # 对features做重新refine\n",
    "        features=refine_decoder(\n",
    "            features,\n",
    "            end_points,\n",
    "            decoder_height=decoder_height,\n",
    "            decoder_widht=decoder_width,\n",
    "            decoder_use_separable_conv=model_options.decoder_use_separable_conv, # 使用离散卷积\n",
    "            model_variant=model_options.model_variant,\n",
    "            weight_decay=weight_decay,\n",
    "            reuse=reuse,\n",
    "            is_training=is_training,\n",
    "            fine_tune_batch_norm=find_tune_batch_norm)\n",
    "        \n",
    "        # 获得batch的logits\n",
    "        # batch_normalization作用:\n",
    "        # 在激活函数之前的bn模块,它接受wx+b计算的feature作为输入.可以做到如下几点:\n",
    "        # 1. 提高梯度传播数度,将所有输出归一化到0~1.避免梯度消失.\n",
    "        # 2. 提高模型的收敛速度.(归一化到0~1,所有的feature都是)\n",
    "        # 3. 减少模型对参数初始化的影响.(归一化到0~1)\n",
    "        outputs_to_logits={}\n",
    "        for output in sorted(model_options.outputs_to_num_classes):\n",
    "            outputs_to_logits[output]=get_batch_logits(\n",
    "                features,\n",
    "                model_options.outputs_to_num_classes[output],\n",
    "                model_options.atrous_rates,\n",
    "                aspp_with_batch_norm=model_options.aspp_with_batch_norm, # batch normalization\n",
    "                weight_decay=weight_decay,\n",
    "                reuse=reuse,\n",
    "                scope_suffix=output) # scope_suffix后缀\n",
    "        \n",
    "        return outputs_to_logits\n",
    "        \n",
    "\n",
    "    \n",
    "def multi_scale_logits(images,\n",
    "                       model_options,\n",
    "                       image_pyramid,\n",
    "                       weight_decay=0.0001,\n",
    "                       is_training=False,\n",
    "                       fine_tune_batch_norm=False):\n",
    "    \"\"\"构建logits方法\n",
    "    args:\n",
    "        model_options: 网络配置的定义信息.\n",
    "        image_pyramid: 图像金字塔\n",
    "        weight_decay: 权重衰减\n",
    "    \"\"\"\n",
    "    if not image_pyramid:\n",
    "        image_pyramid=[1.0] # list\n",
    "    # crop size\n",
    "    crop_height=(\n",
    "        model_options.crop_size[0]\n",
    "        if model_options.crop_size else tf.shape(images)[1])\n",
    "    crop_width=(\n",
    "        model_options.crop_size[1]\n",
    "        if model_options.crop_size else tf.shape(images)[0])\n",
    "    # decoder_output_stride 是在decoder单元,提炼分割结果时候使用的 input/output的比\n",
    "    logits_output_stride=(\n",
    "        model_options.decoder_output_stride or model_options.output_stride)\n",
    "    \n",
    "\n",
    "    logit_height=cal_scaled_dim_val(crop_height,max(1.0,max(image_pyramid))/logits_output_stride) # 这个image_pyramid其实不是很清楚它的含义,是同尺寸的images组还是downsize后images\n",
    "    logit_width=cal_scaled_dim_val(crop_width,max(1.0,max(image_pyramid))/logits_output_stride)   # 2018-09-17: 这个image_pyramide是包含了一组图像缩放的fractor.并不是图像本身.\n",
    "    \n",
    "    outputs_to_scales_to_logits={\n",
    "        k:{}\n",
    "        for k in model_options.outputs_to_num_classes\n",
    "    }\n",
    "    \n",
    "    # step 1 对于每一个缩略图\n",
    "    for image_scale in image_pyramid:\n",
    "        if image_scale!=1.0:\n",
    "            # 不是原图,需要缩放\n",
    "            # 有了缩放因子,需要计算对应的缩放尺寸\n",
    "            scaled_height=cal_scaled_dim_val(crop_height,image_scale)\n",
    "            scaled_width=cal_scaled_dim_val(crop_width,image_scale)\n",
    "            scaled_crop_size=[scaled_height,scaled_width]\n",
    "            # 有了缩放尺寸,需要对原图做缩放了\n",
    "            scaled_images=tf.image.resize_bilinear(images,scaled_crop_size,align_corners=True)\n",
    "            \n",
    "            if model_options.crops_size:\n",
    "                scaled_images.set_shape([None,scaled_height,scale_width,3]) # 如果需要crop size的话,我们把scaled_images reshape成3个chn的.\n",
    "        else:\n",
    "            # 原图\n",
    "            scaled_crop_size=model_options.crop_size\n",
    "            scaled_images=images\n",
    "        \n",
    "        # 用做过scale的尺寸替换参数中的crop_size,然后生成网络\n",
    "        updated_options=model_options._replace(crop_size=scaled_crop_size)\n",
    "        outputs_to_logits=_get_logits(\n",
    "            scaled_images,\n",
    "            updated_options,\n",
    "            weight_decay=weight_decay,\n",
    "            reuse=tf.AUTO_REUSE,\n",
    "            is_training=is_training,\n",
    "            fine_tune_batch_norm=fine_tune_batch_norm)\n",
    "        # 此时拿到结果.对结果做一个reshape,以便和其他的scale pyramid做融合使尺寸是合理的.\n",
    "        for output in sorted(outputs_to_logits):\n",
    "            outputs_to_logits[output]=tf.image.resize_bilinear(\n",
    "                outputs_to_logits[output],\n",
    "                [logit_height,logit_width],\n",
    "                align_corners=True)\n",
    "            \n",
    "        # 只有一层pyramid,就可以返回\n",
    "        if len(image_pyramid)==1:\n",
    "            for output in sorted(model_options.outputs_to_num_classes):\n",
    "                # 第k个scaler fractor对应的LOGITS_SCOPE_NAME,AKA,\"logits\"\n",
    "                outputs_to_scales_to_logits[output][LOGITS_SCOPE_NAME]=outputs_to_logits[output]\n",
    "            \n",
    "            return outputs_to_scales_to_logits\n",
    "        \n",
    "        # 如果有多个pyramid fractor,需要按照对应的标签保存 \n",
    "        for output in sorted(model_options.outputs_to_num_classes):\n",
    "            outputs_to_scales_to_logits[output]['logits_%.2f'%image_scale]=outputs_to_logits[output]\n",
    "            \n",
    "    # 把多个pyramid fractor融合\n",
    "    # 需要新创建一个维度,该维度为了融合使用\n",
    "    for output in model_options.outputs_to_num_classes:\n",
    "        all_logits=[\n",
    "            tf.expand_dims(logits,axis=4)\n",
    "            for logits in outputs_to_scales_to_logits[output].values()\n",
    "        ]\n",
    "        # 在这个新维度上做concat( 理解为连接)\n",
    "        all_logits=tf.concat(all_logits,axis=4)\n",
    "        # 根据不同的融合方法采用不同的tf的融合方法\n",
    "        merge_fn=(\n",
    "            tf.reduce_max\n",
    "            if model_options.merge_method=='max' else tf.reduce_mean)\n",
    "        # 在新增维度上融合.\n",
    "        outputs_to_scales_to_logits[output][MERGED_LOGITS_SCOPE]=merge_fn(all_logits,axis=4)\n",
    "    \n",
    "    return outputs_to_scales_to_logits\n",
    "    \n",
    "                \n",
    "    \n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![image](https://img-blog.csdn.net/20180518234043625?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29KaU1vRGVZZTEyMzQ1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
