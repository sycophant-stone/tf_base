{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'common'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9679494841c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdeeplab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeeplab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeeplab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msegmentation_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'common'"
     ]
    }
   ],
   "source": [
    "\n",
    "import six\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from utils import *\n",
    "from deeplab import common\n",
    "from deeplab import model\n",
    "from deeplab.datasets import segmentation_dataset\n",
    "from deeplab.utils import input_generator\n",
    "from deeplab.utils import train_utils\n",
    "from deployment import model_deploy\n",
    "\n",
    "num_clones= 1 \n",
    "\n",
    "clone_on_cpu= False \n",
    "\n",
    "num_replicas= 1 \n",
    "\n",
    "startup_delay_steps= 15 \n",
    "\n",
    "num_ps_tasks= 0 \n",
    "\n",
    "#master=\n",
    "\n",
    "task= 0 \n",
    "\n",
    "train_logdir= None \n",
    "\n",
    "log_steps= 10 \n",
    "\n",
    "save_interval_secs= 1200 \n",
    "\n",
    "save_summaries_secs= 600 \n",
    "\n",
    "save_summaries_images= False \n",
    "\n",
    "learning_policy= 'poly'\n",
    "\n",
    "base_learning_rate= .0001 \n",
    "\n",
    "learning_rate_decay_factor= 0.1 \n",
    "\n",
    "learning_rate_decay_step= 2000 \n",
    "\n",
    "learning_power= 0.9 \n",
    "\n",
    "training_number_of_steps= 30000 \n",
    "\n",
    "momentum= 0.9\n",
    "\n",
    "train_batch_size= 8 \n",
    "\n",
    "weight_decay= 0.00004 \n",
    "\n",
    "train_crop_size= 523 #[513  513] \n",
    "\n",
    "last_layer_gradient_multiplier= 1.0 \n",
    "\n",
    "upsample_logits= True \n",
    "\n",
    "tf_initial_checkpoint= None \n",
    "\n",
    "initialize_last_layer= True \n",
    "\n",
    "last_layers_contain_logits_only= False \n",
    "\n",
    "slow_start_step= 0 \n",
    "\n",
    "slow_start_learning_rate= 1e-4 \n",
    "\n",
    "fine_tune_batch_norm= True \n",
    "\n",
    "min_scale_factor= 0.5 \n",
    "\n",
    "max_scale_factor= 2. \n",
    "\n",
    "scale_factor_step_size= 0.25 \n",
    "\n",
    "atrous_rates= None \n",
    "\n",
    "output_stride= 16 \n",
    "\n",
    "master=''\n",
    "\n",
    "dataset= 'pascal_voc_seg'\n",
    "\n",
    "train_split= 'train'\n",
    "\n",
    "dataset_dir= \"deeplab/datasets/pascal_voc_seg/tfrecord/cal_train_aug/model.ckpt\"\n",
    "\n",
    "train_logdir=\"deeplab/trainlog\"\n",
    "\n",
    "# --train_logdir=deeplab/      --dataset_dir=deeplab/datasets/pascal_voc_seg/tfrecord/cal_train_aug/model.ckpt     --train_logdir=deeplab/tra  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 输入参数\n",
    "model_variant=\"xception_65\"\n",
    "train_crop_size=513\n",
    "clone_batch_size=train_batch_size//num_clones\n",
    "min_resize_value=None\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "prefetch_queue = slim.prefetch_queue\n",
    "\n",
    "\"\"\"\n",
    "具体的模型建立依赖:\n",
    "common.ModelOptions\n",
    "model.multi_scale_logits\n",
    "train_utils.add_softmax_cross_entropy_loss_for_each_scale\n",
    "\"\"\"\n",
    "\n",
    "def _build_deeplab(inputs_queue,outputs_to_num_classes,ignore_labels):\n",
    "    \"\"\"构建deeplab网络\n",
    "    inputs_queue:\n",
    "            输入sample\n",
    "    outputs_num_classes:\n",
    "            当前是几分类的网络\n",
    "    \n",
    "    return:\n",
    "            返回deeplab网络\n",
    "    \"\"\"\n",
    "    samples=inputs_queue.dequeue() # 从队列中取出样本\n",
    "    # 添加一些助记名字\n",
    "    samples[common.IMAGE]=tf.identity(samples[common.IMAGE],name=common.IMAGE)\n",
    "    samples[common.LABEL]=tf.identity(samples[common.LABEL],name=common.LABEL)\n",
    "    \n",
    "    # setup\n",
    "    model_options=common.ModelOptions(\n",
    "        outputs_to_num_classes=outputs_to_num_classes,\n",
    "        crop_size=train_crop_size,\n",
    "        atrous_rates=atrous_rates,\n",
    "        output_stride=output_stride)\n",
    "    \n",
    "    # 几率表达式(其实是softmax的输出,可认为是概率)\n",
    "    outputs_to_scales_to_logits=model.multi_scale_logits(\n",
    "        samples[common.IMAGE],\n",
    "        model_options=model_options,\n",
    "        image_pyramid=image_pyramid,# image_pyramid=NULL\n",
    "        weight_decay=weight_decay,# 4e-05\n",
    "        is_training=True,\n",
    "        fine_tune_batch_norm=fine_tune_batch_norm)\n",
    "    # 添加一些助记名字\n",
    "    output_type_dict=outputs_to_scales_to_logits[common.OUTPUT_TYPE]\n",
    "    output_type_dict[model.MERGED_LOGITS_SCOPE]=tf.identity(\n",
    "        output_type_dict[model.MERGED_LOGITS_SCOPE],\n",
    "        name=common.OUTPUT_TYPE)\n",
    "    \n",
    "    for output,num_classes in six.iteritems(outputs_to_num_classes):\n",
    "        # softmax\n",
    "        train_utils.add_softmax_cross_entropy_loss_for_each_scale(\n",
    "            outputs_to_scales_to_logits[output],\n",
    "            samples[common.LABEL],\n",
    "            ignore_labels,\n",
    "            loss_weight=1.0,\n",
    "            upsampling_logits=upsample_logits, #Upsample logits during training\n",
    "            scope=output)\n",
    "        \n",
    "    return outputs_to_scales_to_logits\n",
    "        \n",
    "\n",
    "def train():\n",
    "\n",
    "    config=model_deploy.DeploymentConfig(num_clones=num_clones,\n",
    "                                          clone_on_cpu=clone_on_cpu, # bool,是否使用cpu\n",
    "                                          replica_id=task, # task id\n",
    "                                          num_replicas=num_replicas,\n",
    "                                          num_ps_tasks=num_ps_tasks)\n",
    "    assert train_batch_size % num_clones ==0,(\"train batch size SHOULD be divisble by num of cores\")\n",
    "    \n",
    "    # 取出train的数据集\n",
    "    dataSet=segmentation_dataset.get_dataset(dataset,train_split,dataset_dir)\n",
    "    \n",
    "    tf.logging.info(\"this is %s SET\",train_split)\n",
    "    \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        # step 1 把data从磁盘中取出,需要什么,这部分属于input_device.可以使用cpu.因此提供一个tf.devices办法\n",
    "        with tf.device(config.input_device()):\n",
    "            sample=input_generator.get(dataset,\n",
    "                                       train_crop_size,\n",
    "                                       clone_batch_size,\n",
    "                                       min_resize_value=min_resize_value,\n",
    "                                       max_resize_value=max_resize_value,\n",
    "                                       resize_factor=resize_factor,\n",
    "                                       min_scale_factor=min_scale_factor,\n",
    "                                       max_scale_factor=max_scale_factor,\n",
    "                                       scale_factor_step_size=scale_factor_step_size,\n",
    "                                       dataset_split=train_split,\n",
    "                                       is_training=True,\n",
    "                                       model_variant=model_variant)        \n",
    "            prefetch_queue.prefetch_queue(sample,capacity=128*config.num_clones)\n",
    "            \n",
    "        # step 2 对于变量,需要使用gpu.\n",
    "        with tf.device(config.variables_device()):\n",
    "            global_step=tf.train.get_or_create_global_step()\n",
    "            # 定义网络\n",
    "            model_fn=_build_deeplab\n",
    "            model_args=(input_queue,{\n",
    "                common.OUTPUT_TYPE:dataSet.num_classes\n",
    "            },\n",
    "                        dataSet.ignore_label)\n",
    "            # 把deeplab网络图构建起来,构建多个克隆体.\n",
    "            clones=model_deploy.creat_clones(config,model_fn,args=model_args)\n",
    "            \n",
    "            first_clone_scope=config.clone_scope(0) # 是个字符串\"clone_0\"\n",
    "            update_ops=tf.get_collection(tf.GraphKeys.UPDATE_OPS,first_clone_scope) # 找打标记有\"clone_0\"的并且是UPDATE_OPS描述的均值和方差\n",
    "            \n",
    "            \n",
    "        # step 3 在图上构建优化算法\n",
    "        with tf.device(config.optimizer_device()):\n",
    "            learning_rate=train_utils.get_model_learning_rate(learning_policy,\n",
    "                                                              base_learning_rate,\n",
    "                                                              learning_rate_decay_step,\n",
    "                                                              learning_rate_decay_factor,\n",
    "                                                              training_number_of_steps,\n",
    "                                                              learning_power,\n",
    "                                                              slow_start_step,\n",
    "                                                              slow_start_learning_rate\n",
    "                                                             )\n",
    "            optimizer=tf.train.MomentumOptimizer(learning_rate,momentum)\n",
    "        \n",
    "        # step 4 利用optimizer 计算给定clones的loss\n",
    "        with tf.device(config.variables_device()):\n",
    "            total_loss,gradient_and_var=model_deploy.optimize_clones(clones,optimizer)\n",
    "            total_loss=tf.check_numerics(total_loss,'Loss is inf or nan.') # sanity check\n",
    "            \n",
    "            # 更新最后一层的梯度,对于我们可能会对最后一层做fine-tune. 多分类的fine-tune\n",
    "            # 1. 先取出最后一层\n",
    "            last_layers=model.get_extra_layer_scopes(last_layers_contain_logits_only)\n",
    "            grand_mult=train_utils.get_model_gradient_multipliers(last_layers,last_layer_gradient_multiplier)\n",
    "            if grand_mult:\n",
    "                gradient_and_var=slim.learing.multiply_gradients(gradient_and_var,grand_mult)\n",
    "            grand_updates=optimizer.apply_gradients(gradient_and_var,grand_mult)\n",
    "            update_ops.append(grand_updates)\n",
    "            update_ops=tf.group(*update_ops)\n",
    "            with tf.control_dependencies([update_ops]):\n",
    "                train_tensor=tf.identity(total_loss,name='train_op') # train_tensor就是train_op的代名词\n",
    "            \n",
    "        # 允许用cpu\n",
    "        session_config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                      log_device_placement=False)\n",
    "        # step 5 开始train\n",
    "        slim.learing.train(train_tensor,\n",
    "                           logdir=train_logdir,\n",
    "                           log_every_steps=log_steps,\n",
    "                           master=master,\n",
    "                           number_of_steps=training_number_of_steps,\n",
    "                           is_chief=(task==0),\n",
    "                           session_config=session_config,\n",
    "                           init_fn=train_utils.get_model_init_fn(\n",
    "                               train_logdir,\n",
    "                               tf_initial_checkpoint,\n",
    "                               initialize_last_layer,\n",
    "                               last_layers,\n",
    "                               ignore_missing_vars=True,\n",
    "                           ),\n",
    "                           \n",
    "        )\n",
    "\n",
    "train()\n",
    "\n",
    "\"\"\"\n",
    "if __name__=='__main__':\n",
    "    if len(sys.argv) < 2:\n",
    "        print (\"NO action specified.\")\n",
    "        sys.exit()\n",
    "\n",
    "    if sys.argv[1].startswith('--'):\n",
    "        option = sys.argv[1][2:]\n",
    "        if option == 'version':\n",
    "            print (\"version 1.2 \")\n",
    "        elif option == 'help':\n",
    "            print (\"This program prints files to the standard output.\\\n",
    "                 Any number of files can be specified.\\\n",
    "                 Options include:\\\n",
    "                 --version : Prints the version number\\\n",
    "                 --train: traing segnet\\\n",
    "                 --test: test segnet\\\n",
    "                 --help     : Display this help\")\n",
    "            \n",
    "        elif option == 'train':\n",
    "            print(\"start training\")\n",
    "            training(trainfilepath=\"/home/julyedu_433249/work/tf_base/segNet/SegNet/CamVid/train.txt\",\n",
    "             valfilepath=\"/home/julyedu_433249/work/tf_base/segNet/SegNet/CamVid/val.txt\",\n",
    "             batch_size=5,\n",
    "             image_width=480,\n",
    "             image_height=360,\n",
    "             image_ch=3,\n",
    "             max_steps=20000)\n",
    "        elif option == 'test':\n",
    "            print(\"start testing\")\n",
    "            test(testfilename=\"/home/julyedu_433249/work/tf_base/segNet/SegNet/CamVid/test.txt\",\n",
    "             batch_size=5,\n",
    "             image_width=480,\n",
    "             image_height=360,\n",
    "             image_ch=3)\n",
    "\n",
    "        else:\n",
    "            print(\"Unknow option.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONPATH=$PYTHONPATH:$PWD:$PWD/slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/deeplearning/work/tf_base/deepLab\n"
     ]
    }
   ],
   "source": [
    "!echo $PWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/deeplearning/work/tf_base/deepLab/slim\n"
     ]
    }
   ],
   "source": [
    "!echo $PWD/slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {}, 1: {}, 2: {}, 3: {}, 4: {}, 5: {}, 6: {}, 7: {}, 8: {}, 9: {}, 10: {}}\n"
     ]
    }
   ],
   "source": [
    "outputs_to_num_classes=11\n",
    "outputs_to_scales_to_logits = {\n",
    "      k: {}\n",
    "      for k in range(outputs_to_num_classes)\n",
    "  }\n",
    "print(outputs_to_scales_to_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
