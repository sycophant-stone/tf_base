{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-953328a84fa1>, line 172)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-953328a84fa1>\"\u001b[0;36m, line \u001b[0;32m172\u001b[0m\n\u001b[0;31m    model.\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import six\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from utils import *\n",
    "from deeplab import common\n",
    "from deeplab import model\n",
    "from deeplab.datasets import segmentation_dataset\n",
    "from deeplab.utils import input_generator\n",
    "from deeplab.utils import train_utils\n",
    "from deployment import model_deploy\n",
    "\n",
    "num_clones= 1 \n",
    "\n",
    "clone_on_cpu= False \n",
    "\n",
    "num_replicas= 1 \n",
    "\n",
    "startup_delay_steps= 15 \n",
    "\n",
    "num_ps_tasks= 0 \n",
    "\n",
    "#master=\n",
    "\n",
    "task= 0 \n",
    "\n",
    "train_logdir= None \n",
    "\n",
    "log_steps= 10 \n",
    "\n",
    "save_interval_secs= 1200 \n",
    "\n",
    "save_summaries_secs= 600 \n",
    "\n",
    "save_summaries_images= False \n",
    "\n",
    "learning_policy= 'poly'\n",
    "\n",
    "base_learning_rate= .0001 \n",
    "\n",
    "learning_rate_decay_factor= 0.1 \n",
    "\n",
    "learning_rate_decay_step= 2000 \n",
    "\n",
    "learning_power= 0.9 \n",
    "\n",
    "training_number_of_steps= 30000 \n",
    "\n",
    "momentum= 0.9\n",
    "\n",
    "train_batch_size= 8 \n",
    "\n",
    "weight_decay= 0.00004 \n",
    "\n",
    "train_crop_size= 523 #[513  513] \n",
    "\n",
    "last_layer_gradient_multiplier= 1.0 \n",
    "\n",
    "upsample_logits= True \n",
    "\n",
    "tf_initial_checkpoint= None \n",
    "\n",
    "initialize_last_layer= True \n",
    "\n",
    "last_layers_contain_logits_only= False \n",
    "\n",
    "slow_start_step= 0 \n",
    "\n",
    "slow_start_learning_rate= 1e-4 \n",
    "\n",
    "fine_tune_batch_norm= True \n",
    "\n",
    "min_scale_factor= 0.5 \n",
    "\n",
    "max_scale_factor= 2. \n",
    "\n",
    "scale_factor_step_size= 0.25 \n",
    "\n",
    "atrous_rates= None \n",
    "\n",
    "output_stride= 16 \n",
    "\n",
    "dataset= 'pascal_voc_seg'\n",
    "\n",
    "train_split= 'train'\n",
    "\n",
    "dataset_dir= \"deeplab/datasets/pascal_voc_seg/tfrecord/cal_train_aug/model.ckpt\"\n",
    "\n",
    "train_logdir=\"deeplab/trainlog\"\n",
    "\n",
    "# --train_logdir=deeplab/      --dataset_dir=deeplab/datasets/pascal_voc_seg/tfrecord/cal_train_aug/model.ckpt     --train_logdir=deeplab/tra  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 输入参数\n",
    "model_variant=\"xception_65\"\n",
    "train_crop_size=513\n",
    "clone_batch_size=train_batch_size//num_clones\n",
    "min_resize_value=None\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "prefetch_queue = slim.prefetch_queue\n",
    "\n",
    "def train():\n",
    "\n",
    "    config=model_deploy.DeploymentConfig(num_clones=num_clones,\n",
    "                                          clone_on_cpu=clone_on_cpu, # bool,是否使用cpu\n",
    "                                          replica_id=task, # task id\n",
    "                                          num_replicas=num_replicas,\n",
    "                                          num_ps_tasks=num_ps_tasks)\n",
    "    assert train_batch_size % num_clones ==0,(\"train batch size SHOULD be divisble by num of cores\")\n",
    "    \n",
    "    # 取出train的数据集\n",
    "    dataSet=segmentation_dataset.get_dataset(dataset,train_split,dataset_dir)\n",
    "    \n",
    "    tf.logging.info(\"this is %s SET\",train_split)\n",
    "    \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        # step 1 把data从磁盘中取出,需要什么,这部分属于input_device.可以使用cpu.因此提供一个tf.devices办法\n",
    "        with tf.device(config.input_device()):\n",
    "            sample=input_generator.get(dataset,\n",
    "                                       train_crop_size,\n",
    "                                       clone_batch_size,\n",
    "                                       min_resize_value=min_resize_value,\n",
    "                                       max_resize_value=max_resize_value,\n",
    "                                       resize_factor=resize_factor,\n",
    "                                       min_scale_factor=min_scale_factor,\n",
    "                                       max_scale_factor=max_scale_factor,\n",
    "                                       scale_factor_step_size=scale_factor_step_size,\n",
    "                                       dataset_split=train_split,\n",
    "                                       is_training=True,\n",
    "                                       model_variant=model_variant)        \n",
    "            prefetch_queue.prefetch_queue(sample,capacity=128*config.num_clones)\n",
    "            \n",
    "        # step 2 对于变量,需要使用gpu.\n",
    "        with tf.device(config.variables_device()):\n",
    "            global_step=tf.train.get_or_create_global_step()\n",
    "            # 定义网络\n",
    "            model_fn=_build_deeplab\n",
    "            model_args=(input_queue,{\n",
    "                common.OUTPUT_TYPE:dataSet.num_classes\n",
    "            },\n",
    "                        dataSet.ignore_label)\n",
    "            # 把deeplab网络图构建起来,构建多个克隆体.\n",
    "            clones=model_deploy.creat_clones(config,model_fn,args=model_args)\n",
    "            \n",
    "            first_clone_scope=config.clone_scope(0) # 是个字符串\"clone_0\"\n",
    "            update_ops=tf.get_collection(tf.GraphKeys.UPDATE_OPS,first_clone_scope) # 找打标记有\"clone_0\"的并且是UPDATE_OPS描述的均值和方差\n",
    "            \n",
    "            \n",
    "        # step 3 在图上构建优化算法\n",
    "        with tf.device(config.optimizer_device()):\n",
    "            learning_rate=train_utils.get_model_learning_rate(learning_policy,\n",
    "                                                              base_learning_rate,\n",
    "                                                              learning_rate_decay_step,\n",
    "                                                              learning_rate_decay_factor,\n",
    "                                                              training_number_of_steps,\n",
    "                                                              learning_power,\n",
    "                                                              slow_start_step,\n",
    "                                                              slow_start_learning_rate\n",
    "                                                             )\n",
    "            optimizer=tf.train.MomentumOptimizer(learning_rate,momentum)\n",
    "        \n",
    "        # step 4 利用optimizer 计算给定clones的loss\n",
    "        with tf.device(config.variables_device()):\n",
    "            total_loss,gradient_and_var=model_deploy.optimize_clones(clones,optimizer)\n",
    "            total_loss=tf.check_numerics(total_loss,'Loss is inf or nan.') # sanity check\n",
    "            \n",
    "            # 更新最后一层的梯度,对于我们可能会对最后一层做fine-tune. 多分类的fine-tune\n",
    "            # 1. 先取出最后一层\n",
    "            last_layers=model.get_extra_layer_scopes(last_layers_contain_logits_only)\n",
    "            train_utils.get_model_gradient_multipliers(last_layers,last_layer_gradient_multiplier)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "train()\n",
    "\n",
    "\"\"\"\n",
    "if __name__=='__main__':\n",
    "    if len(sys.argv) < 2:\n",
    "        print (\"NO action specified.\")\n",
    "        sys.exit()\n",
    "\n",
    "    if sys.argv[1].startswith('--'):\n",
    "        option = sys.argv[1][2:]\n",
    "        if option == 'version':\n",
    "            print (\"version 1.2 \")\n",
    "        elif option == 'help':\n",
    "            print (\"This program prints files to the standard output.\\\n",
    "                 Any number of files can be specified.\\\n",
    "                 Options include:\\\n",
    "                 --version : Prints the version number\\\n",
    "                 --train: traing segnet\\\n",
    "                 --test: test segnet\\\n",
    "                 --help     : Display this help\")\n",
    "            \n",
    "        elif option == 'train':\n",
    "            print(\"start training\")\n",
    "            training(trainfilepath=\"/home/julyedu_433249/work/tf_base/segNet/SegNet/CamVid/train.txt\",\n",
    "             valfilepath=\"/home/julyedu_433249/work/tf_base/segNet/SegNet/CamVid/val.txt\",\n",
    "             batch_size=5,\n",
    "             image_width=480,\n",
    "             image_height=360,\n",
    "             image_ch=3,\n",
    "             max_steps=20000)\n",
    "        elif option == 'test':\n",
    "            print(\"start testing\")\n",
    "            test(testfilename=\"/home/julyedu_433249/work/tf_base/segNet/SegNet/CamVid/test.txt\",\n",
    "             batch_size=5,\n",
    "             image_width=480,\n",
    "             image_height=360,\n",
    "             image_ch=3)\n",
    "\n",
    "        else:\n",
    "            print(\"Unknow option.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONPATH=$PYTHONPATH:$PWD:$PWD/slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/deeplearning/work/tf_base/deepLab\n"
     ]
    }
   ],
   "source": [
    "!echo $PWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/deeplearning/work/tf_base/deepLab/slim\n"
     ]
    }
   ],
   "source": [
    "!echo $PWD/slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
