{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 284)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m284\u001b[0m\n\u001b[0;31m    startstep=0 # 如果是finetune的话不为0.\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "   This program is for SegNet.\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "import time\n",
    "from PIL import Image\n",
    "from math import ceil\n",
    "from tensorflow.python.ops import gen_nn_ops\n",
    "# modules\n",
    "#from Utils import _variable_with_weight_decay, _variable_on_cpu, _add_loss_summaries, _activation_summary, print_hist_summery, get_hist, per_class_acc, writeImage\n",
    "#from Inputs import *\n",
    "\n",
    "'''-------'''\n",
    "# parma presetting\n",
    "IMAGE_HEIGHT = 360\n",
    "IMAGE_WIDTH = 480\n",
    "IMAGE_DEPTH = 3\n",
    "NUM_CLASSES=12\n",
    "\n",
    "# Constants describing the training process.\n",
    "MOVING_AVERAGE_DECAY = 0.9999     # The decay to use for the moving average.\n",
    "NUM_EPOCHS_PER_DECAY = 350.0      # Epochs after which learning rate decays.\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.\n",
    "\n",
    "INITIAL_LEARNING_RATE = 0.001 \n",
    "\n",
    "'''------------------------------------------------------------------'''\n",
    "##input\n",
    "\n",
    "def get_filename_list(filename):\n",
    "\t# 拿到数据集\n",
    "\tfd =open(filename)\n",
    "\tfor line in fd:\n",
    "\t\tline=line.strip().split(\" \")\n",
    "\t\timage_path=line[0]\n",
    "\t\tlabel_path=line[1]\n",
    "\t\n",
    "\treturn image_path,label_path\n",
    "\n",
    "def CamVidInput(inputdatafilename,inputlabelfilename,batch_size):\n",
    "    \"\"\"\n",
    "    inputdatafilename: 输入data\n",
    "    inputlabelfilename: 输入y\n",
    "    batch_size: 一次计算的量\n",
    "    读到文件名队列中,然后读出图片.reshape\n",
    "    \"\"\"\n",
    "    # 把数据集的文件名也保存成张量模式. 须知tensorflow就是对张量的操作.\n",
    "    images=tf.convert_to_tensor(inputdatafilename,dtype=dtypes.string)\n",
    "    labels=tf.convert_to_tensor(inputlabelfilename,dtype=dtypes.string)\n",
    "    \n",
    "    # 把这些数据集名字读入到内存中.\n",
    "    filename_queue=tf.train.slice_input_producer([images,labels],shuffle=True)\n",
    "    image_val=tf.read_file(filename_queue[0])\n",
    "    label_val=tf.read_file(filename_queue[1])\n",
    "    \n",
    "    # 数据集是png图片,解析.\n",
    "    image_bytes=tf.image.decode_png(image_val)\n",
    "    label_bytes=tf.image.decode_png(label_val)\n",
    "    \n",
    "    # 把读到的png图片做reshape\n",
    "    image=tf.reshape(image_bytes,(IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH))\n",
    "    label=tf.reshape(label_bytes,(IMAGE_HEIGHT,IMAGE_WIDTH,1))\n",
    "    \n",
    "    return image,label\n",
    "    \n",
    "'''------------------------------------------------------------------'''\n",
    "# Networks\n",
    "\n",
    "def orthogonal_initializer(scale=1.1) # 正交\n",
    "    ''' From Lasagne and Keras. Reference: Saxe et al., http://arxiv.org/abs/1312.6120\n",
    "    '''\n",
    "    def _initializer(shape,dtype=tf.float32,partition_info=None):\n",
    "        print(\"[%]:  shape[0]:%s, shape[1]:shape[1]\"%(\"orthogonal_initializer\",shape[0],shape[1]))\n",
    "        flat_shape=(shape[0],np.prod(shape[1,:]))\n",
    "        a=np.random.normal(0.0,1.0,flat_shape)\n",
    "        u,_,v=np.linalg.svd(a,full_matrices=False)\n",
    "        q=u if u.shape==flat_shape else v\n",
    "        q=q.reshape(shape)\n",
    "        return tf.constant(scale*q[:,shape[0],:shape[1]],dtype=tf.float32)\n",
    "    return _initializer\n",
    "        \n",
    "\n",
    "def helper_variable_on_cpu(name,shape,initializer):\n",
    "    with tf.device('/gpu:0'):\n",
    "        var=tf.get_variable(name,shape,initializer=initializer)\n",
    "    return var\n",
    "\n",
    "def helper_variable_with_weight_decay(name,shape,initializer,wd):\n",
    "    var=helper_variable_on_cpu(name,shape,initializer)\n",
    "    if wd is not None:\n",
    "        weight_decay=tf.multiply(tf.nn.l2_loss(var),wd,name=\"weight_loss\")\n",
    "        tf.add_to_collection(\"losses\",weight_decay)\n",
    "    return var\n",
    "\n",
    "def helper_add_loss_summaries(total_loss):\n",
    "    loss_average=tf.train.ExponentialMovingAverage(0.9,name='avg')\n",
    "    losses=tf.get_collection('losses')\n",
    "    loss_average_op=loss_average.apply(losses+[total_loss])\n",
    "    \n",
    "    \"\"\" \n",
    "    summary\n",
    "    for l in losses+[total_loss]:\n",
    "        \n",
    "    \"\"\"\n",
    "    return loss_average_op\n",
    "    \n",
    "def batch_normal_layer(inputI,is_trainning,scope):\n",
    "    return tf.cond(is_trainning,\n",
    "                   lambda: tf.contrib.layers.batch_norm(inputI,is_trainning=True,center=False,updates_collections=None,scope=scope+\"_bn\"),\n",
    "                   lambda: tf.contrib.layers.batch_norm(inputI,is_trainning=False,center=False,updates_collections=None,scope=scope+\"_bn\",reuse=True))\n",
    "\n",
    "def conv_layer_with_bn(inputT,shape,train_shape,activation=True,name=None):\n",
    "    in_ch=shape[2]\n",
    "    out_ch=shape[3]\n",
    "    k_size=shape[0]\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        kernel=helper_variable_with_weight_decay(\"ort_weights\",shape=shape,initializer=orthogonal_initializer,wd=None)\n",
    "        conv=tf.nn.conv2d(inputI,kernel,[1,1,1,1],padding='SAME')# 1x1 kernel size, 1 batch , 1 chn\n",
    "        biases=helper_variable_on_cpu('biases',[out_ch],tf.constant_initializer(0.0))\n",
    "        bias= tf.nn.bias_add(conv,biases)\n",
    "        if activation is True:\n",
    "            conv_out=tf.nn.relu()\n",
    "        \n",
    "\n",
    "def get_decode_filter(f_shape):\n",
    "    \"\"\"\n",
    "    reference: https://github.com/MarvinTeichmann/tensorflow-fcn\n",
    "    \"\"\"\n",
    "    width=f_shape[0]\n",
    "    height=f_shape[0]\n",
    "    f=ceil(width/2.0)\n",
    "    c=(2*f-1-f%2)/(2.0*f)\n",
    "    bilinear=np.zeros([f_shape[0],f_shape[1]])\n",
    "    # 双线性插值\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            value =(1-np.abs(x/f-c))*(1-np.abs(y/f-c))\n",
    "            bilinear[x,y]=value\n",
    "    weights=np.zeros(f_shape)\n",
    "    for i in range(f_shape[2]):\n",
    "        weights[:,:,i,i]=bilinear\n",
    "    init=tf.constant_initializer(value=weights,dtype=tf.float32)\n",
    "    return tf.get_variable(name=\"up_filter\",initializer=init,shape=weights.shape)\n",
    "        \n",
    "        \n",
    "def decode_layer(inputI,f_shape,output_shape,stride=2,name=None):\n",
    "    # 这个shape的格式: kernel_w,kernel_h,batch,chn\n",
    "    sess_temp=tf.global_variables_initializer()\n",
    "    strides=[1,stride,stride,1]\n",
    "    with tf.variable_scope(name):\n",
    "        weights=get_decode_filter(f_shape)\n",
    "        deconv=tf.nn.conv2d_transpose(inputI,weights,output_shape,stride=stride,padding='SAME')\n",
    "    \n",
    "    return deconv\n",
    "\n",
    "def msra_initializer(k1,d1):\n",
    "    \"\"\"\n",
    "    k1: kernel size\n",
    "    d1: filter number\n",
    "    \"\"\"\n",
    "    stddev=math.sqrt(2.0/(k1**2*d1))\n",
    "    return tf.truncated_normal_initializer(stddev=stddev)\n",
    "\n",
    "def weight_loss(logits,labels,num_classes,head=None):\n",
    "    \"\"\"\n",
    "    median-frequency re-weighting\n",
    "    \"\"\"\n",
    "    with tf.name_scope('loss'):\n",
    "        logits=tf.reshape(logits,(-1,num_classes))\n",
    "        epsilon=tf.constant(value=1e-10)\n",
    "        logits=logits+epsilon\n",
    "        label_flat=tf.reshape(labels,(-1,1))\n",
    "        labels=tf.reshape(tf.one_hot(label_flat,depth=num_classes),(-1,num_classes))\n",
    "        softmax=tf.nn.softmax(logits)\n",
    "        cross_entropy=tf.reduce_sum(tf.multiply(labels*tf.log(softmax+epsilon),head),axis=[1])\n",
    "        cross_entropy_mean=tf.reduce_mean(cross_entropy,name='cross_entropy')\n",
    "        tf.add_to_collection('losses',cross_entropy_mean)\n",
    "        loss=tf.add_n(tf.get_collection('losses'),name='total_loss')\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def cal_loss(logits,labels):\n",
    "    loss_weight = np.array([\n",
    "      0.2595,\n",
    "      0.1826,\n",
    "      4.5640,\n",
    "      0.1417,\n",
    "      0.9051,\n",
    "      0.3826,\n",
    "      9.6446,\n",
    "      1.8418,\n",
    "      0.6823,\n",
    "      6.2478,\n",
    "      7.3614,\n",
    "      1.0974]) # class 0~11\n",
    "    labels=tf.cast(labels,tf.int32)\n",
    "    return weight_loss(logits,labels,NUM_CLASSES,head=loss_weight)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "def interface(images,labels,batch_size,phase_train):\n",
    "    \"\"\"\n",
    "    images,labels: \n",
    "                读入的数据集\n",
    "    batch_size:\n",
    "                按批次的做训练\n",
    "    phase_train:\n",
    "                是个Bool类型.\n",
    "    \"\"\"\n",
    "    # local response normalization\n",
    "    norm1=tf.nn.lrn(images,depth_radius=5,bias=1.0,alpha=0.0001,beta=0.75,name=\"norm1\")\n",
    "    # conv1\n",
    "    conv1=conv_layer_with_bn(norm1,[7,7,images.get_shape().as_list()[3],64],phase_train,name=\"conv1\")\n",
    "    # pool1\n",
    "    pool1,pool1_indices=tf.nn.max_pool_with_argmax(conv1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME',name='pool1')\n",
    "    \n",
    "    # conv2\n",
    "    conv2=conv_layer_with_bn(pool1,[7,7,64,64],phase_train,name='conv2')\n",
    "    # pool2\n",
    "    pool2,pool2_indices=tf.nn.max_pool_with_argmax(conv2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME',name='pool2')\n",
    "    \n",
    "    # conv3\n",
    "    conv3=conv_layer_with_bn(pool2,[7,7,64,64],phase_train,name='conv3')\n",
    "    # pool3\n",
    "    pool3,pool3_indices=tf.nn.max_pool_with_argmax(conv3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME',name='pool3')\n",
    "    \n",
    "    # conv4\n",
    "    conv4=conv_layer_with_bn(pool3,[7,7,64,64],phase_train,name='conv4')\n",
    "    # pool3\n",
    "    pool4,pool4_indices=tf.nn.max_pool_with_argmax(conv4,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME',name='pool4')\n",
    "    \"\"\" end of encoder\"\"\"\n",
    "    \n",
    "    \"\"\"upsampling\"\"\"\n",
    "    \n",
    "    # upsamping4\n",
    "    upsample4=decode_layer(pool4,[2,2,64,64],[batch_size,45,60,64],2,\"up4\")\n",
    "    # decode4\n",
    "    conv_decode4=conv_layer_with_bn(upsample4,[7,7,64,64],False,\"conv_decode4\")\n",
    "    \n",
    "    # upsamping3\n",
    "    upsample3=decode_layer(conv_decode4,[2,2,64,64],[batch_size,90,120,64],2,\"up3\")\n",
    "    # decode3\n",
    "    conv_decode3=conv_layer_with_bn(upsample3,[7,7,64,64],False,\"conv_decode3\")\n",
    "    \n",
    "    # upsamping2\n",
    "    upsample2=decode_layer(conv_decode3,[2,2,64,64],[batch_size,180,240,64],2,\"up2\")\n",
    "    # decode2\n",
    "    conv_decode2=conv_layer_with_bn(upsample2,[7,7,64,64],False,\"conv_decode2\")\n",
    "    \n",
    "    # upsampling1\n",
    "    upsample1=decode_layer(conv_decode2,[2,2,64,64],[batch_size,360,480,64],2,\"up1\")\n",
    "    # decode2\n",
    "    conv_decode1=conv_layer_with_bn(upsample1,[7,7,64,64],False,\"conv_decode1\")\n",
    "    \n",
    "    \"\"\" end of decode\"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\" classify\"\"\"\n",
    "    \n",
    "    with tf.variable_scope('conv_classifier') as scope:\n",
    "        kernel=helper_variable_with_weight_decay(\"weights\",shape=[1,1,64,NUM_CLASSES],initializer=msra_initializer(1,64),\n",
    "                                                wd=0.0005)\n",
    "        conv=tf.nn.conv2d(conv_decode1,kernel,[1,1,1,1],padding='SAME')\n",
    "        biases=helper_variable_on_cpu('biases',[NUM_CLASSES],tf.constant_initializer(0.0))\n",
    "        conv_classifier=tf.nn.bias_add(conv,biases,name=scope.name)\n",
    "    \n",
    "    logit=conv_classifier\n",
    "    loss=cal_loss(conv_classifier,labels)\n",
    "    \n",
    "    return loss,logit\n",
    "    \n",
    "\n",
    "# train's network\n",
    "def train(total_loss,global_step):\n",
    "    total_sample=274\n",
    "    num_batches_per_epoch = 274/1\n",
    "    lr=INITIAL_LEARNING_RATE\n",
    "    loss_average_op=helper_add_loss_summaries(total_loss=total_loss)\n",
    "    \n",
    "    # gradiens\n",
    "    with tf.control_dependencies([loss_average_op]):\n",
    "        opt=tf.train.AdamOptimizer(lr)\n",
    "        grads=opt.compute_gradients(total_loss)\n",
    "    \n",
    "    apply_gradient_op=opt.apply_gradients(grads,global_step=global_step)\n",
    "    \n",
    "    variable_averages=tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)\n",
    "    variable_averages_op=variable_averages.appy(tf.trainable_variables())\n",
    "    \n",
    "    with tf.control_dependencies([apply_gradient_op,variable_averages_op]):\n",
    "        train_op=tf.no_op(name='train')\n",
    "        \n",
    "    return train_op\n",
    "    \n",
    "    \n",
    "    \n",
    "'''------------------------------------------------------------------'''\n",
    "\n",
    "def training(trainfilepath,valfilepath,batch_size,image_width,image_height,image_ch,max_steps):\n",
    "\ttrain_image_filenames,train_label_filenames=get_filename_list(trainfilepath)\n",
    "\tval_image_filenames,val_label_filenames=get_filename_list(valfilepath)\n",
    "\t\n",
    "    startstep=0 # 如果是finetune的话不为0.\n",
    "    with tf.Graph().as_default():\n",
    "        # train_data_node和train_label_node\n",
    "        #    这两者作为训练集的data和label.\n",
    "        train_data_node=tf.placeholder(tf.float32,shape=[batch_size,image_height,image_width,image_ch])\n",
    "        train_label_node=tf.placeholder(tf.float32,shape=[batch_size,image_height,image_width,1]) # 它是1个通道.\n",
    "\n",
    "        #phase_train\n",
    "        # phase_train作为conv*的输入.是一个True和false的\n",
    "        phase_train=tf.placeholder(tf.float32,name=\"phase_train\") # 为什么没有设置shape=[],因为它是Bool型变量\n",
    "        global_step=tf.variable(0,trainable=False) # 设置步长,不参与训练\n",
    "        \n",
    "        \n",
    "        # 读出camVid\n",
    "        train_images,train_labels=CamVidInput(train_image_filenames,train_label_filenames,batch_size)\n",
    "        val_images,val_labels=CamVidInput(val_image_filenames,val_label_filenames,batch_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 建立encoder+decoder的网络图\n",
    "        #     输入: data和y.\n",
    "        #     返回损失和预测精度\n",
    "        #     phase_train作用,是Bool型\n",
    "        # train_data_node,train_label_node: \n",
    "        #     输入数据集及标签.\n",
    "        loss,eval_prediction=inference(train_data_node,train_label_node,batch_size,phase_train)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 建立train的图\n",
    "        train_op=train(loss,global_step)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            init=tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "            \n",
    "            # 创建线程,并用coordinator()管理\n",
    "            coord=tf.train.Coordinator()\n",
    "            threads=tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "            \n",
    "            \n",
    "            for step in range(startstep,startstep+max_steps):\n",
    "                image_batch,label_batch=sess.run([train_images,train_labels])\n",
    "                feed_dict={\n",
    "                    train_data_node:image_batch,\n",
    "                    train_label_node:label_batch,\n",
    "                    phase_train:True\n",
    "                }\n",
    "                _,loss_value=sess.run([train_op,loss],feed_dict=feed_dict) # 第一个_是不关心的.\n",
    "                if step%10==0:\n",
    "                    print(\"setp:%d,loss=%.2f\" %(step,loss_value))\n",
    "                pred=sess.run(eval_prediction,feed_dict=feed_dict)\n",
    "                print(\"pred:%s\"pred)\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "\n",
    "if __name__=='__main__':\n",
    "    training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "train.txt的内容.\n",
    "包含有需要的图片名.\n",
    "```txt\n",
    "SegNet/CamVid/train/0001TP_006690.png SegNet/CamVid/trainannot/0001TP_006690.png\n",
    "SegNet/CamVid/train/0001TP_006720.png SegNet/CamVid/trainannot/0001TP_006720.png\n",
    "SegNet/CamVid/train/0001TP_006750.png SegNet/CamVid/trainannot/0001TP_006750.png\n",
    "SegNet/CamVid/train/0001TP_006780.png SegNet/CamVid/trainannot/0001TP_006780.png\n",
    "SegNet/CamVid/train/0001TP_006810.png SegNet/CamVid/trainannot/0001TP_006810.png\n",
    "SegNet/CamVid/train/0001TP_006840.png SegNet/CamVid/trainannot/0001TP_006840.png\n",
    "SegNet/CamVid/train/0001TP_006870.png SegNet/CamVid/trainannot/0001TP_006870.png\n",
    "SegNet/CamVid/train/0001TP_006900.png SegNet/CamVid/trainannot/0001TP_006900.png\n",
    "SegNet/CamVid/train/0001TP_006930.png SegNet/CamVid/trainannot/0001TP_006930.png\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
