{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/julyedu_433249/work/tf_base/segNet/SegNet/CamVid/train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0da2a5ae2711>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    665\u001b[0m              \u001b[0mimage_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m360\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m              \u001b[0mimage_ch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m              max_steps=20000)\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-0da2a5ae2711>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(trainfilepath, valfilepath, batch_size, image_width, image_height, image_ch, max_steps)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_width\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_ch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m     \u001b[0mtrain_image_filenames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label_filenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_filename_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0mval_image_filenames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_label_filenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_filename_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0mstartstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-0da2a5ae2711>\u001b[0m in \u001b[0;36mget_filename_list\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_filename_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# 拿到数据集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mimage_filenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mlabel_filenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/julyedu_433249/work/tf_base/segNet/SegNet/CamVid/train.txt'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "   This program is for SegNet.\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "import time\n",
    "from PIL import Image\n",
    "from math import ceil\n",
    "from tensorflow.python.ops import gen_nn_ops\n",
    "import skimage\n",
    "# modules\n",
    "#from Utils import _variable_with_weight_decay, _variable_on_cpu, _add_loss_summaries, _activation_summary, print_hist_summery, get_hist, per_class_acc, writeImage\n",
    "#from Inputs import *\n",
    "\n",
    "'''-------'''\n",
    "# parma presetting\n",
    "IMAGE_HEIGHT = 360\n",
    "IMAGE_WIDTH = 480\n",
    "IMAGE_DEPTH = 3\n",
    "NUM_CLASSES=12\n",
    "\n",
    "EVAL_BATCH_SIZE = 5\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "# Constants describing the training process.\n",
    "MOVING_AVERAGE_DECAY = 0.9999     # The decay to use for the moving average.\n",
    "NUM_EPOCHS_PER_DECAY = 350.0      # Epochs after which learning rate decays.\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.\n",
    "\n",
    "INITIAL_LEARNING_RATE = 0.001 \n",
    "\n",
    "\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 367\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TEST = 101\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 1\n",
    "TEST_ITER = NUM_EXAMPLES_PER_EPOCH_FOR_TEST / BATCH_SIZE\n",
    "\n",
    "'''------------------------------------------------------------------'''\n",
    "##input\n",
    "def fast_hist(a, b, n):\n",
    "    k = (a >= 0) & (a < n)\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n**2).reshape(n, n)\n",
    "\n",
    "def per_class_acc(predictions, label_tensor):\n",
    "    labels = label_tensor\n",
    "    size = predictions.shape[0]\n",
    "    num_class = predictions.shape[3]\n",
    "    hist = np.zeros((num_class, num_class))\n",
    "    for i in range(size):\n",
    "      hist += fast_hist(labels[i].flatten(), predictions[i].argmax(2).flatten(), num_class)\n",
    "    acc_total = np.diag(hist).sum() / hist.sum()\n",
    "    print ('accuracy = %f'%np.nanmean(acc_total))\n",
    "    iu = np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))\n",
    "    print ('mean IU  = %f'%np.nanmean(iu))\n",
    "    for ii in range(num_class):\n",
    "        if float(hist.sum(1)[ii]) == 0:\n",
    "          acc = 0.0\n",
    "        else:\n",
    "          acc = np.diag(hist)[ii] / float(hist.sum(1)[ii])\n",
    "        print(\"    class # %d accuracy = %f \"%(ii,acc))\n",
    "\n",
    "def get_filename_list(filename):\n",
    "\t# 拿到数据集\n",
    "\tfd =open(filename)\n",
    "\timage_filenames=[]\n",
    "\tlabel_filenames=[]\n",
    "\tfor line in fd:\n",
    "\t\tline=line.strip().split(\" \")\n",
    "\t\timage_filenames.append(line[0])\n",
    "\t\tlabel_filenames.append(line[1])\n",
    "\t\n",
    "\treturn image_filenames,label_filenames\n",
    "\n",
    "def helper_generate_image_label_batch(images,labels,min_queue_examples,batch_size,shuffle=True):\n",
    "    \"\"\"\n",
    "    有image和label以及batch_size.拿出一组 [btach_size,ih,iw,ch]的数据\n",
    "    \"\"\" \n",
    "    print(\"min_queue_examples:\",min_queue_examples)\n",
    "    if shuffle:\n",
    "        images_batch,label_batch=tf.train.shuffle_batch([images,labels],batch_size=batch_size,num_threads=1,capacity=min_queue_examples+3*batch_size,min_after_dequeue=min_queue_examples)\n",
    "    else:\n",
    "        images,label_batch=tf.train.batch([images,labels],batch_size=batch_size,num_threads=1,capacity=min_queue_examples+3*batch_size)\n",
    "    print(\"label_batch shape:%d,%d,%d,%d\"%(label_batch.shape[0],label_batch.shape[1],label_batch.shape[2],label_batch.shape[3]))\n",
    "    return images_batch,label_batch\n",
    "\n",
    "\n",
    "def CamVidInput(inputdatafilename,inputlabelfilename,batch_size):\n",
    "    \"\"\"\n",
    "    inputdatafilename: 输入data\n",
    "    inputlabelfilename: 输入y\n",
    "    batch_size: 一次计算的量\n",
    "    读到文件名队列中,然后读出图片.reshape\n",
    "    \"\"\"\n",
    "    # 把数据集的文件名也保存成张量模式. 须知tensorflow就是对张量的操作.\n",
    "    images=ops.convert_to_tensor(inputdatafilename,dtype=dtypes.string)\n",
    "    labels=ops.convert_to_tensor(inputlabelfilename,dtype=dtypes.string)\n",
    "    print(\"images:\",images)\n",
    "    print(\"labels:\",labels)\n",
    "    # 把这些数据集名字读入到内存中.\n",
    "    filename_queue=tf.train.slice_input_producer([images,labels],shuffle=True)\n",
    "    image_val=tf.read_file(filename_queue[0])\n",
    "    label_val=tf.read_file(filename_queue[1])\n",
    "    \n",
    "    # 数据集是png图片,解析.\n",
    "    image_bytes=tf.image.decode_png(image_val)\n",
    "    label_bytes=tf.image.decode_png(label_val)\n",
    "    \n",
    "    # 把读到的png图片做reshape\n",
    "    image=tf.reshape(image_bytes,(IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_DEPTH))\n",
    "    label=tf.reshape(label_bytes,(IMAGE_HEIGHT,IMAGE_WIDTH,1))\n",
    "    # 丢掉了\n",
    "    reshape_image=tf.cast(image,tf.float32)\n",
    "    \n",
    "    # 产生一组images和labels\n",
    "    min_fraction_of_example_in_queue=0.4\n",
    "    min_queue_examples=int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN * min_fraction_of_example_in_queue)\n",
    "    return helper_generate_image_label_batch(reshape_image,label,min_queue_examples,batch_size,shuffle=True)\n",
    "    \n",
    "def get_all_test_data(image_filenames,label_filenames):\n",
    "    \"\"\"根据image和label文件名路径,读取出对应文件,把它们整理成数据.\n",
    "    args:\n",
    "        image_filenames:\n",
    "            测试集文件路径\n",
    "        label_filenames:\n",
    "            测试集的mask图,这个作为预测的真值.\n",
    "    return:\n",
    "        返回数组状态的images和labels.\n",
    "    \"\"\"\n",
    "    images=[]\n",
    "    labels=[]\n",
    "    for im_filename,lb_filename in zip(image_filenames,label_filenames):\n",
    "        im=np.array(skimage.io.imread(im_filename))\n",
    "        im=im[np.newaxis] # 其实起到一个把数组变成矩阵的功能\n",
    "        images.append(im)\n",
    "        lb=skimage.io.imread(lb_filename)\n",
    "        lb=lb[np.newaxis]\n",
    "        lb=lb[...,np.newaxis]\n",
    "        labels.append(lb)\n",
    "    \n",
    "    return images,labels\n",
    "\n",
    "\n",
    "def writeImage(image, filename):\n",
    "    \"\"\" store label data to colored image \"\"\"\n",
    "    Sky = [128,128,128]\n",
    "    Building = [128,0,0]\n",
    "    Pole = [192,192,128]\n",
    "    Road_marking = [255,69,0]\n",
    "    Road = [128,64,128]\n",
    "    Pavement = [60,40,222]\n",
    "    Tree = [128,128,0]\n",
    "    SignSymbol = [192,128,128]\n",
    "    Fence = [64,64,128]\n",
    "    Car = [64,0,128]\n",
    "    Pedestrian = [64,64,0]\n",
    "    Bicyclist = [0,128,192]\n",
    "    Unlabelled = [0,0,0]\n",
    "    r = image.copy()\n",
    "    g = image.copy()\n",
    "    b = image.copy()\n",
    "    label_colours = np.array([Sky, Building, Pole, Road_marking, Road, Pavement, Tree, SignSymbol, Fence, Car, Pedestrian, Bicyclist, Unlabelled])\n",
    "    for l in range(0,12):\n",
    "        \"\"\"\n",
    "        image是预测值,它应该有一组区域构成,同一组区块用一个数字表示,比如0,会表示sky.比如靠近地面的地方会有一组4,表示road\n",
    "        但是这个数字图是不能显示出来给人看的,那么我们可以针对每一组都一个它一个颜色,比如对于所有位置是1的区域,人为修改该区域的\n",
    "        rbg为sky的Sky = [128,128,128],这样这种图可以被人眼看出来.\n",
    "        处理时候还是会依照rgb来处理\n",
    "        \"\"\"\n",
    "        r[image==l]=label_colours[l,0]\n",
    "        g[image==l]=label_colours[l,1]\n",
    "        b[image==l]=label_colours[l,2]\n",
    "  \n",
    "    # 保存图片\n",
    "    rgb=np.zeros([image.shape[0],image.shape[1],3])\n",
    "    rgb[:,:,0]=r/1.0\n",
    "    rgb[:,:,1]=g/1.0\n",
    "    rgb[:,:,2]=b/1.0\n",
    "    im=Image.fromarray(np.uint8(rgb))\n",
    "    im.save(filename)\n",
    "\n",
    "    \n",
    "'''------------------------------------------------------------------'''\n",
    "# Networks\n",
    "\n",
    "def orthogonal_initializer(scale=1.1): # 正交\n",
    "    ''' From Lasagne and Keras. Reference: Saxe et al., http://arxiv.org/abs/1312.6120\n",
    "    '''\n",
    "    def _initializer(shape,dtype=tf.float32,partition_info=None):\n",
    "        print(\"[%s]:  shape[0]:%s, shape[1]:%s\"%(\"orthogonal_initializer\",shape[0],shape[1]))\n",
    "        flat_shape=(shape[0],np.prod(shape[1:])) # 这里shape[1:] 不是shape[1,:],shape本身并不是多维的\n",
    "        a=np.random.normal(0.0,1.0,flat_shape)\n",
    "        u,_,v=np.linalg.svd(a,full_matrices=False)\n",
    "        q=u if u.shape==flat_shape else v\n",
    "        q=q.reshape(shape)\n",
    "        return tf.constant(scale*q[:shape[0],:shape[1]],dtype=tf.float32) # 0:shape[0]\n",
    "    return _initializer\n",
    "        \n",
    "\n",
    "def helper_variable_on_cpu(name,shape,initializer):\n",
    "    with tf.device('/gpu:0'):\n",
    "        var=tf.get_variable(name,shape,initializer=initializer)\n",
    "    return var\n",
    "\n",
    "def helper_variable_with_weight_decay(name,shape,initializer,wd):\n",
    "    var=helper_variable_on_cpu(name,shape,initializer)\n",
    "    if wd is not None:\n",
    "        weight_decay=tf.multiply(tf.nn.l2_loss(var),wd,name=\"weight_loss\")\n",
    "        tf.add_to_collection(\"losses\",weight_decay)\n",
    "    return var\n",
    "\n",
    "def helper_add_loss_summaries(total_loss):\n",
    "    loss_average=tf.train.ExponentialMovingAverage(0.9,name='avg')\n",
    "    losses=tf.get_collection('losses')\n",
    "    loss_average_op=loss_average.apply(losses+[total_loss])\n",
    "    \n",
    "    \"\"\" \n",
    "    summary\n",
    "    for l in losses+[total_loss]:\n",
    "        \n",
    "    \"\"\"\n",
    "    for l in losses+[total_loss]:\n",
    "        tf.summary.scalar(l.op.name+' (raw)', l)\n",
    "        tf.summary.scalar(l.op.name, loss_average.average(l))\n",
    "\n",
    "    return loss_average_op\n",
    "    \n",
    "def batch_norm_layer(inputI,is_trainning,scope):\n",
    "    return tf.cond(is_trainning,\n",
    "                   lambda: tf.contrib.layers.batch_norm(inputI,is_training=True,center=False,updates_collections=None,scope=scope+\"_bn\"),\n",
    "                   lambda: tf.contrib.layers.batch_norm(inputI,is_training=False,center=False,updates_collections=None,scope=scope+\"_bn\",reuse=True))\n",
    "\n",
    "def conv_layer_with_bn(inputI,shape,train_phase,activation=True,name=None):\n",
    "    in_ch=shape[2]\n",
    "    out_ch=shape[3]\n",
    "    k_size=shape[0]\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        kernel=helper_variable_with_weight_decay(\"ort_weights\",shape=shape,initializer=orthogonal_initializer(),wd=None) # 注意此处orthogonal_initializer一定是该函数的返回值\n",
    "        conv=tf.nn.conv2d(inputI,kernel,[1,1,1,1],padding='SAME')# 1x1 kernel size, 1 batch , 1 chn\n",
    "        biases=helper_variable_on_cpu('biases',[out_ch],tf.constant_initializer(0.0))\n",
    "        bias= tf.nn.bias_add(conv,biases)\n",
    "        if activation is True:\n",
    "            conv_out=tf.nn.relu(batch_norm_layer(bias,train_phase,scope.name))\n",
    "        else:\n",
    "            conv_out=batch_norm_layer(bias,train_phase,scope.name)\n",
    "    return conv_out\n",
    "\n",
    "        \n",
    "\n",
    "def get_decode_filter(f_shape):\n",
    "    \"\"\"\n",
    "    reference: https://github.com/MarvinTeichmann/tensorflow-fcn\n",
    "    \"\"\"\n",
    "    width=f_shape[0]\n",
    "    height=f_shape[0]\n",
    "    f=ceil(width/2.0)\n",
    "    c=(2*f-1-f%2)/(2.0*f)\n",
    "    bilinear=np.zeros([f_shape[0],f_shape[1]])\n",
    "    # 双线性插值\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            value =(1-np.abs(x/f-c))*(1-np.abs(y/f-c))\n",
    "            bilinear[x,y]=value\n",
    "    weights=np.zeros(f_shape)\n",
    "    for i in range(f_shape[2]):\n",
    "        weights[:,:,i,i]=bilinear\n",
    "    init=tf.constant_initializer(value=weights,dtype=tf.float32)\n",
    "    return tf.get_variable(name=\"up_filter\",initializer=init,shape=weights.shape)\n",
    "        \n",
    "        \n",
    "def deconv_layer(inputI,f_shape,output_shape,stride=2,name=None):\n",
    "    # 这个shape的格式: kernel_w,kernel_h,batch,chn\n",
    "    sess_temp=tf.global_variables_initializer()\n",
    "    strides=[1,stride,stride,1]\n",
    "    with tf.variable_scope(name):\n",
    "        weights=get_decode_filter(f_shape)\n",
    "        deconv=tf.nn.conv2d_transpose(inputI,weights,output_shape,strides=strides,padding='SAME') # 这里strides区别于stride,strides是一组参数.\n",
    "    \n",
    "    return deconv\n",
    "\n",
    "def msra_initializer(k1,d1):\n",
    "    \"\"\"\n",
    "    k1: kernel size\n",
    "    d1: filter number\n",
    "    \"\"\"\n",
    "    stddev=math.sqrt(2. /(k1**2 * d1))\n",
    "    return tf.truncated_normal_initializer(stddev=stddev)\n",
    "\n",
    "def weight_loss(logits,labels,num_classes,head=None):\n",
    "    \"\"\"\n",
    "    median-frequency re-weighting\n",
    "    \"\"\"\n",
    "    with tf.name_scope('loss'):\n",
    "        logits=tf.reshape(logits,(-1,num_classes))\n",
    "        epsilon=tf.constant(value=1e-10)\n",
    "        logits=logits+epsilon\n",
    "        label_flat=tf.reshape(labels,(-1,1))\n",
    "        labels=tf.reshape(tf.one_hot(label_flat,depth=num_classes),(-1,num_classes))\n",
    "        softmax=tf.nn.softmax(logits)\n",
    "        \"\"\"[[[特别注意]]]: 交叉熵是是正值,由于概率是小于1的,所以log的值是负值,所以需要一个负号来修正\"\"\"\n",
    "        cross_entropy=  -tf.reduce_sum(tf.multiply(labels*tf.log(softmax+epsilon),head),axis=[1]) ### 注意交叉熵是正值,log操作前面要有负号来修证!!!!!!\n",
    "        cross_entropy_mean=tf.reduce_mean(cross_entropy,name='cross_entropy')\n",
    "        tf.add_to_collection('losses',cross_entropy_mean)\n",
    "        loss=tf.add_n(tf.get_collection('losses'),name='total_loss')\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def cal_loss(logits,labels):\n",
    "    loss_weight = np.array([\n",
    "      0.2595,\n",
    "      0.1826,\n",
    "      4.5640,\n",
    "      0.1417,\n",
    "      0.9051,\n",
    "      0.3826,\n",
    "      9.6446,\n",
    "      1.8418,\n",
    "      0.6823,\n",
    "      6.2478,\n",
    "      7.3614,\n",
    "      1.0974]) # class 0~11\n",
    "    labels=tf.cast(labels,tf.int32)\n",
    "    return weight_loss(logits,labels,NUM_CLASSES,head=loss_weight)\n",
    "\n",
    "\n",
    "def get_hist(predictions,labels):# labels不准确,这里传入的是batch_size\n",
    "    num_cls=predictions.shape[3]\n",
    "    batch_size=predictions.shape[0]\n",
    "    hist=np.zeros([num_cls,num_cls])\n",
    "    for i in range(batch_size):\n",
    "        ylabel=labels[i].flatten()\n",
    "        #print(\"ylabel\",ylabel)\n",
    "        yHat=predictions[i].argmax(2).flatten()\n",
    "        #print(\"yHat\",yHat)\n",
    "        k=(ylabel>0)&(ylabel<num_cls) # sanity check , 找到同时满足0<?<num_cls的那些位的地址.\n",
    "        #print(\"k\",k)\n",
    "        #print(\"len(k)\",len(k))\n",
    "        #print(\"labels.len\",len(ylabel))\n",
    "        #print(\"yHat.len\",len(yHat))\n",
    "        hist+=np.bincount(num_cls*ylabel[k].astype(int)+yHat[k],minlength=num_cls**2).reshape(num_cls,num_cls) # num_cls个类出现的次数.\n",
    "    \n",
    "    return hist\n",
    "        \n",
    "        \n",
    "\n",
    "def inference(images,labels,batch_size,phase_train):\n",
    "    \"\"\"\n",
    "    images,labels: \n",
    "                读入的数据集\n",
    "    batch_size:\n",
    "                按批次的做训练\n",
    "    phase_train:\n",
    "                是个Bool类型.\n",
    "    \"\"\"\n",
    "    # local response normalization\n",
    "    norm1=tf.nn.lrn(images,depth_radius=5,bias=1.0,alpha=0.0001,beta=0.75,name=\"norm1\")\n",
    "    # conv1\n",
    "    conv1=conv_layer_with_bn(norm1,[7,7,images.get_shape().as_list()[3],64],phase_train,name=\"conv1\")\n",
    "    # pool1\n",
    "    pool1,pool1_indices=tf.nn.max_pool_with_argmax(conv1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME',name='pool1')\n",
    "    \n",
    "    # conv2\n",
    "    conv2=conv_layer_with_bn(pool1,[7,7,64,64],phase_train,name='conv2')\n",
    "    # pool2\n",
    "    pool2,pool2_indices=tf.nn.max_pool_with_argmax(conv2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME',name='pool2')\n",
    "    \n",
    "    # conv3\n",
    "    conv3=conv_layer_with_bn(pool2,[7,7,64,64],phase_train,name='conv3')\n",
    "    # pool3\n",
    "    pool3,pool3_indices=tf.nn.max_pool_with_argmax(conv3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME',name='pool3')\n",
    "    \n",
    "    # conv4\n",
    "    conv4=conv_layer_with_bn(pool3,[7,7,64,64],phase_train,name='conv4')\n",
    "    # pool3\n",
    "    pool4,pool4_indices=tf.nn.max_pool_with_argmax(conv4,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME',name='pool4')\n",
    "    \"\"\" end of encoder\"\"\"\n",
    "    \n",
    "    \"\"\"upsampling\"\"\"\n",
    "    \n",
    "    # upsamping4\n",
    "    upsample4=deconv_layer(pool4,[2,2,64,64],[batch_size,45,60,64],2,\"up4\")\n",
    "    # decode4\n",
    "    conv_decode4=conv_layer_with_bn(upsample4,[7,7,64,64],phase_train,False,\"conv_decode4\")\n",
    "    \n",
    "    # upsamping3\n",
    "    upsample3=deconv_layer(conv_decode4,[2,2,64,64],[batch_size,90,120,64],2,\"up3\")\n",
    "    # decode3\n",
    "    conv_decode3=conv_layer_with_bn(upsample3,[7,7,64,64],phase_train,False,\"conv_decode3\")\n",
    "    \n",
    "    # upsamping2\n",
    "    upsample2=deconv_layer(conv_decode3,[2,2,64,64],[batch_size,180,240,64],2,\"up2\")\n",
    "    # decode2\n",
    "    conv_decode2=conv_layer_with_bn(upsample2,[7,7,64,64],phase_train,False,\"conv_decode2\")\n",
    "    \n",
    "    # upsampling1\n",
    "    upsample1=deconv_layer(conv_decode2,[2,2,64,64],[batch_size,360,480,64],2,\"up1\")\n",
    "    # decode2\n",
    "    conv_decode1=conv_layer_with_bn(upsample1,[7,7,64,64],phase_train,False,\"conv_decode1\")\n",
    "    \n",
    "    \"\"\" end of decode\"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\" classify\"\"\"\n",
    "    \n",
    "    with tf.variable_scope('conv_classifier') as scope:\n",
    "        kernel=helper_variable_with_weight_decay(\"weights\",shape=[1,1,64,NUM_CLASSES],initializer=msra_initializer(1,64),\n",
    "                                                wd=0.0005)\n",
    "        conv=tf.nn.conv2d(conv_decode1,kernel,[1,1,1,1],padding='SAME')\n",
    "        biases=helper_variable_on_cpu('biases',[NUM_CLASSES],tf.constant_initializer(0.0))\n",
    "        conv_classifier=tf.nn.bias_add(conv,biases,name=scope.name)\n",
    "    \n",
    "    logit=conv_classifier\n",
    "    loss=cal_loss(conv_classifier,labels)\n",
    "    \n",
    "    return loss,logit,norm1\n",
    "    \n",
    "\n",
    "# train's network\n",
    "def train(total_loss,global_step):\n",
    "    total_sample=274\n",
    "    num_batches_per_epoch = 274/1\n",
    "    lr=INITIAL_LEARNING_RATE\n",
    "    loss_average_op=helper_add_loss_summaries(total_loss=total_loss)\n",
    "    \n",
    "    # gradiens\n",
    "    with tf.control_dependencies([loss_average_op]):\n",
    "        opt=tf.train.AdamOptimizer(lr)\n",
    "        grads=opt.compute_gradients(total_loss)\n",
    "    \n",
    "    apply_gradient_op=opt.apply_gradients(grads,global_step=global_step)\n",
    "    \n",
    "    # 记录参与训练的变量的直方图\n",
    "    for var in tf.trainable_variables():\n",
    "        print(\"%s\",var.op.name)\n",
    "        tf.summary.histogram(var.op.name,var)\n",
    "    \n",
    "    # 记录梯度直方图\n",
    "    for grad,var in grads:\n",
    "        if grad is not None:\n",
    "            stringname=var.op.name+\"/gradients\"\n",
    "            print(stringname)\n",
    "            tf.summary.histogram(var.op.name+\"/gradients\",grad)\n",
    "        \n",
    "    \n",
    "    variable_averages=tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)\n",
    "    variable_averages_op=variable_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "    with tf.control_dependencies([apply_gradient_op,variable_averages_op]):\n",
    "        train_op=tf.no_op(name='train')\n",
    "        \n",
    "    return train_op\n",
    "\n",
    "'''------------------------------------------------------------------'''\n",
    "\n",
    "def test(testfilepath,batch_size,image_width,image_height,image_ch):\n",
    "    \"\"\"测试函数,会恢复training时候保存的ckpt模型文件\n",
    "    testfilepath:\n",
    "        含有测试集的照片列表文件\n",
    "    batch_size:\n",
    "        每次做几个文件的推断\n",
    "    image_width,image_height,image_ch\n",
    "        每张图的尺寸\n",
    "    \n",
    "    \"\"\"\n",
    "    # 读取测试集,分成image和label.\n",
    "    test_image_filenames,test_label_filenames=get_filename_list(testfilepath)\n",
    "    \n",
    "    # 建立graph的测试节点\n",
    "    test_data_node=tf.placeholder(tf.float32,shape=[batch_size,image_height,image_width,image_ch])\n",
    "    test_label_node=tf.placeholder(tf.int64,shape=[batch_size,360,480,1]) # 只有单通道的mask图.\n",
    "    phase_train=tf.placeholder(tf.bool,name=\"phase_train\")\n",
    "    \n",
    "    # 给网络通入输入\n",
    "    loss,logit=inference(test_data_node,test_label_node,batch_size,phase_train)\n",
    "    \n",
    "    pred=tf.argmax(logit,axis=3) # 取出预测最高的前三个结果\n",
    "    \n",
    "    # 从保存的网络中直接恢复到当前网络的权重中,而非恢复到影子变量中.\n",
    "    variable_arverage=tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
    "    variable_restore=variable_arverage.variables_to_restore()\n",
    "    saver=tf.train.Saver(variable_restore)\n",
    "    \n",
    "    # 开始往网络中冠数据\n",
    "    with tf.Session() as sess:\n",
    "        # 加载网络\n",
    "        saver.restore(sess,\"segnet.model.ckpt\")\n",
    "        \n",
    "        # 加载图片和mask的label图\n",
    "        # 区别于train时候利用tensorflow去读数据集,我们这里利用skimage包去读测试集\n",
    "        test_images,test_labels=get_all_test_data(test_image_filenames,test_label_filenames)\n",
    "        \n",
    "        # 启动文件名队列填充过程\n",
    "        tf.train.start_queue_runners(sess)\n",
    "        \n",
    "        # 统计正确率需要的hist\n",
    "        hist= np.zeros((NUM_CLASSES,NUM_CLASSES))\n",
    "        \n",
    "        # 批量\n",
    "        for image_batch,label_batch in zip(test_images,test_labels): # 注意理解和trian时候利用sess.run得到数据集的不同.\n",
    "            feed_dict={\n",
    "                test_data_node:image_batch,\n",
    "                test_label_node:label_batch,\n",
    "                phase_train:False\n",
    "            }\n",
    "            test_loss,test_logit,test_pred=sess.run([loss,logit,pred],feed_dict=feed_dict)\n",
    "            \n",
    "            # 保存预测区域\n",
    "            writeImage(test_pred[0],\"test_pred.png\") # 保存第一个最大的预测\n",
    "            hist+=get_hist(test_logit,label_batch)\n",
    "        # 一组batch推断后,评估准确率\n",
    "        acc_total=np.diag(hist).sum()/hist.sum()\n",
    "        # 交并区域\n",
    "        iu=np.diag(hist).sum()/(hist.sum(0)+hist.sum(1)-np.diag(hist).sum())\n",
    "        print(\"acc:\",acc_total)\n",
    "        print(\"mean iu\",np.mean(iu))\n",
    "    \n",
    "'''------------------------------------------------------------------'''\n",
    "\n",
    "def training(trainfilepath,valfilepath,batch_size,image_width,image_height,image_ch,max_steps):\n",
    "    train_image_filenames,train_label_filenames=get_filename_list(trainfilepath)\n",
    "    val_image_filenames,val_label_filenames=get_filename_list(valfilepath)\n",
    "    startstep = 0\n",
    "    with tf.Graph().as_default():\n",
    "        # train_data_node和train_label_node\n",
    "        #    这两者作为训练集的data和label.\n",
    "        train_data_node=tf.placeholder(tf.float32,shape=[batch_size,image_height,image_width,image_ch])\n",
    "        train_label_node=tf.placeholder(tf.float32,shape=[batch_size,image_height,image_width,1]) # 它是1个通道.\n",
    "\n",
    "        #phase_train\n",
    "        # phase_train作为conv*的输入.是一个True和false的\n",
    "        phase_train=tf.placeholder(tf.bool,name=\"phase_train\") # 为什么没有设置shape=[],因为它是Bool型变量,  phase_train是个bool型而非float32型.它描述当前是train还是test.true为train.\n",
    "        global_step=tf.Variable(0,trainable=False) # 设置步长,不参与训练\n",
    "        \n",
    "        \n",
    "        # 读出camVid\n",
    "        train_images,train_labels=CamVidInput(train_image_filenames,train_label_filenames,batch_size)\n",
    "        val_images,val_labels=CamVidInput(val_image_filenames,val_label_filenames,batch_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 建立encoder+decoder的网络图\n",
    "        #     输入: data和y.\n",
    "        #     返回损失和预测精度\n",
    "        #     phase_train作用,是Bool型\n",
    "        # train_data_node,train_label_node: \n",
    "        #     输入数据集及标签.\n",
    "        loss,eval_prediction,norm1=inference(train_data_node,train_label_node,batch_size,phase_train)\n",
    "        \n",
    "        # 建立train的图\n",
    "        train_op=train(loss,global_step)\n",
    "        \n",
    "        summary_op=tf.summary.merge_all() #把之前train的summary合到一起.\n",
    "        \n",
    "        # 记录average_loss,test_accuracy,mean_IU值,它们是标量.\n",
    "        average_loss =tf.placeholder(tf.float32)\n",
    "        accuracy=tf.placeholder(tf.float32)\n",
    "        mean_iu=tf.placeholder(tf.float32)\n",
    "        \n",
    "       \n",
    "        saver=tf.train.Saver(tf.global_variables())\n",
    "        \n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            init=tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "            \n",
    "            # 创建线程,并用coordinator()管理\n",
    "            coord=tf.train.Coordinator()\n",
    "            threads=tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "            # tf.summary.scalar 会存储标量\n",
    "            average_loss_smy=tf.summary.scalar(\"average_loss\",average_loss)\n",
    "            accuracy_smy=tf.summary.scalar(\"accuracy\",accuracy)\n",
    "            mean_iu_smy=tf.summary.scalar(\"mean_iu\",mean_iu)\n",
    "        \n",
    "            # tf file writer\n",
    "            logdir=\"/home/julyedu_433249/work/log\"\n",
    "            summary_filewriter=tf.summary.FileWriter(logdir,sess.graph)\n",
    "            \n",
    "            \n",
    "            for step in range(startstep,startstep+max_steps):\n",
    "                image_batch,label_batch=sess.run([train_images,train_labels])\n",
    "                feed_dict={\n",
    "                    train_data_node:image_batch,\n",
    "                    train_label_node:label_batch,\n",
    "                    phase_train:True\n",
    "                }\n",
    "                #print(\"train_data_node shape\",train_data_node.shape())\n",
    "                #print(\"image_batch shape\",image_batch.shape())\n",
    "                _,loss_value=sess.run([train_op,loss],feed_dict=feed_dict) # 第一个_是不关心的.\n",
    "                if step==0:\n",
    "                    print(\"setp:%d,loss=%.2f\" %(step,loss_value))\n",
    "                    \n",
    "                if step%10==0:\n",
    "                    print(\"setp:%d,loss=%.2f\" %(step,loss_value))\n",
    "                    pred=sess.run(eval_prediction,feed_dict=feed_dict)\n",
    "                    per_class_acc(pred,label_batch)\n",
    "                    \"\"\"For debug\n",
    "                    norm1_val=sess.run(norm1,feed_dict=feed_dict)\n",
    "                    print(\"norm1_val\",norm1_val)\n",
    "                    \"\"\"\n",
    "                if step%100==0:#每100次做一下验证集.计算误差,精度等.\n",
    "                    #print(\"pred:%s\"%pred)\n",
    "                    print(\"start validation\")\n",
    "                    total_val_loss=0.0\n",
    "                    hist=np.zeros([NUM_CLASSES,NUM_CLASSES])\n",
    "                    for val_step in range(int(TEST_ITER)):\n",
    "                        val_image_batch,val_label_batch=sess.run([val_images,val_labels])\n",
    "                        val_loss,val_pred=sess.run([loss,eval_prediction],feed_dict={\n",
    "                            train_data_node:val_image_batch,\n",
    "                            train_label_node:val_label_batch,\n",
    "                            phase_train:True\n",
    "                        })\n",
    "                        #print(\"val_pred shape:%d,%d,%d,%d\"%(val_pred.shape[0],val_pred.shape[1],val_pred.shape[2],val_pred.shape[3]))\n",
    "                        #print(\"val_image_batch shape:%d,%d,%d,%d\"%(val_image_batch.shape[0],val_image_batch.shape[1],val_image_batch.shape[2],val_image_batch.shape[3]))\n",
    "                        total_val_loss+=val_loss\n",
    "                        hist+=get_hist(val_pred,val_label_batch) #val_image_batch)  注意此处是val_label_batch,而非image.这个花了我们好长时间.\n",
    "                    print(\"hist:\",hist)\n",
    "                    print(\"np.diag(hist)\",np.diag(hist))\n",
    "                    print(\"hist.sum(0)\",hist.sum(0))\n",
    "                    print(\"hist.sum(1)\",hist.sum(1))\n",
    "                    acc_total=np.diag(hist).sum()/hist.sum()\n",
    "                    iu=np.diag(hist)/(hist.sum(0)+hist.sum(1)-np.diag(hist))\n",
    "                    average_loss_smy_str=sess.run(average_loss_smy,feed_dict={\n",
    "                        average_loss:total_val_loss/TEST_ITER\n",
    "                    })\n",
    "                    accuracy_smy_str=sess.run(accuracy_smy,feed_dict={\n",
    "                        accuracy:acc_total\n",
    "                    })\n",
    "                    mean_iu_smy_str=sess.run(mean_iu_smy,feed_dict={\n",
    "                        mean_iu:np.nanmean(iu) # 注意这里有np.nanmean对iu的均值.不包含nan的数值.\n",
    "                    })\n",
    "                    summary_str=sess.run(summary_op,feed_dict=feed_dict)\n",
    "                    summary_filewriter.add_summary(summary_str,step)\n",
    "                    summary_filewriter.add_summary(average_loss_smy_str,step)\n",
    "                    summary_filewriter.add_summary(accuracy_smy_str,step)\n",
    "                    summary_filewriter.add_summary(mean_iu_smy_str,step)\n",
    "                \n",
    "                if step%1000==0 or (step+1)==max_steps:\n",
    "                    # 保存模型\n",
    "                    checkpoint_path=os.path.join(logdir,\"segnet.model.ckpt\")\n",
    "                    saver.save(sess,checkpoint_path,global_step=step)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    if len(sys.argv) < 2:\n",
    "        print (\"NO action specified.\")\n",
    "        sys.exit()\n",
    "\n",
    "    if sys.argv[1].startswith('--'):\n",
    "        option = sys.argv[1][2:]\n",
    "        if option == 'version':\n",
    "            print (\"version 1.2 \")\n",
    "        elif option == 'help':\n",
    "            print (\"This program prints files to the standard output.\\\n",
    "                 Any number of files can be specified.\\\n",
    "                 Options include:\\\n",
    "                 --version : Prints the version number\\\n",
    "                 --train: traing segnet\\\n",
    "                 --test: test segnet\\\n",
    "                 --help     : Display this help\")\n",
    "            \n",
    "        elif option == 'train':\n",
    "            print(\"start training\")\n",
    "            training(trainfilepath=\"/home/julyedu_433249/work/tf_base/segNet/SegNet/CamVid/train.txt\",\n",
    "             valfilepath=\"/home/julyedu_433249/work/tf_base/segNet/SegNet/CamVid/val.txt\",\n",
    "             batch_size=5,\n",
    "             image_width=480,\n",
    "             image_height=360,\n",
    "             image_ch=3,\n",
    "             max_steps=20000)\n",
    "        elif option == 'test':\n",
    "            print(\"start testing\")\n",
    "            test(testfilename=\"/home/julyedu_433249/work/tf_base/segNet/SegNet/CamVid/test.txt\",\n",
    "             batch_size=5,\n",
    "             image_width=480,\n",
    "             image_height=360,\n",
    "             image_ch=3)\n",
    "\n",
    "        else:\n",
    "            print(\"Unknow option.\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "train.txt的内容.\n",
    "包含有需要的图片名.\n",
    "```txt\n",
    "SegNet/CamVid/train/0001TP_006690.png SegNet/CamVid/trainannot/0001TP_006690.png\n",
    "SegNet/CamVid/train/0001TP_006720.png SegNet/CamVid/trainannot/0001TP_006720.png\n",
    "SegNet/CamVid/train/0001TP_006750.png SegNet/CamVid/trainannot/0001TP_006750.png\n",
    "SegNet/CamVid/train/0001TP_006780.png SegNet/CamVid/trainannot/0001TP_006780.png\n",
    "SegNet/CamVid/train/0001TP_006810.png SegNet/CamVid/trainannot/0001TP_006810.png\n",
    "SegNet/CamVid/train/0001TP_006840.png SegNet/CamVid/trainannot/0001TP_006840.png\n",
    "SegNet/CamVid/train/0001TP_006870.png SegNet/CamVid/trainannot/0001TP_006870.png\n",
    "SegNet/CamVid/train/0001TP_006900.png SegNet/CamVid/trainannot/0001TP_006900.png\n",
    "SegNet/CamVid/train/0001TP_006930.png SegNet/CamVid/trainannot/0001TP_006930.png\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
